{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183a7685",
   "metadata": {},
   "source": [
    "# Implementation of Multilayer Perceptron from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4457fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933e526",
   "metadata": {},
   "source": [
    "# 4.2.1 Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242541c",
   "metadata": {},
   "source": [
    "Recall that Fashion-MNIST contains 10 classes, and that each image consists of a 28x28=784\n",
    " grid of grayscale pixel values. Again, we will disregard the spatial structure among the pixels for now, so we can think of this as simply a classification dataset with 784 input features and 10 classes. To begin, we will implement an MLP with one hidden layer and 256 hidden units. Note that we can regard both of these quantities as hyperparameters. Typically, we choose layer widths in powers of 2, which tend to be computationally efficient because of how memory is allocated and addressed in hardware.\n",
    "\n",
    "Again, we will represent our parameters with several tensors. Note that for every layer, we must keep track of one weight matrix and one bias vector. As always, we allocate memory for the gradients of the loss with respect to these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0461e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs = 784, 10\n",
    "num_hiddens1 = 256\n",
    "num_hiddens2 = 128   # new second hidden layer\n",
    "\n",
    "# Layer 1: Input → Hidden1\n",
    "W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens1) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens1))\n",
    "\n",
    "# Layer 2: Hidden1 → Hidden2\n",
    "W2 = nn.Parameter(torch.randn(num_hiddens1, num_hiddens2) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_hiddens2))\n",
    "\n",
    "# Layer 3: Hidden2 → Output\n",
    "W3 = nn.Parameter(torch.randn(num_hiddens2, num_outputs) * 0.01)\n",
    "b3 = nn.Parameter(torch.zeros(num_outputs))\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1765e6",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec84f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e988ed",
   "metadata": {},
   "source": [
    "# 4.2.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30538e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X @ W1 + b1)  # Here '@' stands for matrix multiplication\n",
    "    return (H @ W2 + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecd9a9",
   "metadata": {},
   "source": [
    "\n",
    "# 4.2.4 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd9ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26949477",
   "metadata": {},
   "source": [
    "# 4.2.5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c454d751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.8689 | Train Acc: 0.7016 | Test Acc: 0.7876\n",
      "Epoch 2/10 | Loss: 0.5015 | Train Acc: 0.8182 | Test Acc: 0.8248\n",
      "Epoch 2/10 | Loss: 0.5015 | Train Acc: 0.8182 | Test Acc: 0.8248\n",
      "Epoch 3/10 | Loss: 0.4519 | Train Acc: 0.8363 | Test Acc: 0.8282\n",
      "Epoch 3/10 | Loss: 0.4519 | Train Acc: 0.8363 | Test Acc: 0.8282\n",
      "Epoch 4/10 | Loss: 0.4152 | Train Acc: 0.8503 | Test Acc: 0.8322\n",
      "Epoch 4/10 | Loss: 0.4152 | Train Acc: 0.8503 | Test Acc: 0.8322\n",
      "Epoch 5/10 | Loss: 0.3902 | Train Acc: 0.8589 | Test Acc: 0.8496\n",
      "Epoch 5/10 | Loss: 0.3902 | Train Acc: 0.8589 | Test Acc: 0.8496\n",
      "Epoch 6/10 | Loss: 0.3748 | Train Acc: 0.8654 | Test Acc: 0.8535\n",
      "Epoch 6/10 | Loss: 0.3748 | Train Acc: 0.8654 | Test Acc: 0.8535\n",
      "Epoch 7/10 | Loss: 0.3622 | Train Acc: 0.8691 | Test Acc: 0.8529\n",
      "Epoch 7/10 | Loss: 0.3622 | Train Acc: 0.8691 | Test Acc: 0.8529\n",
      "Epoch 8/10 | Loss: 0.3490 | Train Acc: 0.8747 | Test Acc: 0.8414\n",
      "Epoch 8/10 | Loss: 0.3490 | Train Acc: 0.8747 | Test Acc: 0.8414\n",
      "Epoch 9/10 | Loss: 0.3417 | Train Acc: 0.8763 | Test Acc: 0.8596\n",
      "Epoch 9/10 | Loss: 0.3417 | Train Acc: 0.8763 | Test Acc: 0.8596\n",
      "Epoch 10/10 | Loss: 0.3288 | Train Acc: 0.8808 | Test Acc: 0.8353\n",
      "Epoch 10/10 | Loss: 0.3288 | Train Acc: 0.8808 | Test Acc: 0.8353\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_sum = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for X, y in train_iter:\n",
    "        # Forward pass\n",
    "        y_pred = net(X)\n",
    "        l = loss(y_pred, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        updater.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss_sum += l.item()\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        train_total += y.size(0)\n",
    "        train_correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            y_pred = net(X)\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            test_total += y.size(0)\n",
    "            test_correct += (predicted == y).sum().item()\n",
    "    \n",
    "    train_acc = train_correct / train_total\n",
    "    test_acc = test_correct / test_total\n",
    "    avg_loss = train_loss_sum / len(train_iter)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
