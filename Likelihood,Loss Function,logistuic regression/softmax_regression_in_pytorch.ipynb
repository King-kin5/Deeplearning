{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd22188",
   "metadata": {},
   "source": [
    "# Softmax Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7edf9",
   "metadata": {},
   "source": [
    "We use the Fashion-MNIST data set with batch size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35caa5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:13<00:00, 2.01MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 197kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:04<00:00, 990kB/s] \n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 6.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 256\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs, lr = 5, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade487eb",
   "metadata": {},
   "source": [
    "## The Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653faf60",
   "metadata": {},
   "source": [
    "We can now define the softmax function. For that we rst exponentiate each term using exp\n",
    " and then sum each row to get the normalization constant. Last we divide each row by its normalization constant and return the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4498018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition # The broadcast mechanism is applied here.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47108476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3210, 0.1204, 0.2654, 0.1163, 0.1770],\n",
      "        [0.0546, 0.0445, 0.2507, 0.6212, 0.0289]]) tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "X = torch.normal(0, 1, size=(2, 5))\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, X_prob.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9beb9b5",
   "metadata": {},
   "source": [
    "## The Model and Parameters Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f821f2",
   "metadata": {},
   "source": [
    "Since each example is an image with 28x28\n",
    " pixels we can store it as a 784 dimensional vector. Moreover, since we have 10 categories, the single layer network has an output dimension of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40a40aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten()\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,784)\n",
    "\n",
    "net = nn.Sequential(Flatten(), nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # Initialize weight parameter by a normal distribition \n",
    "        # with a mean of 0 and standard deviation of 0.01.\n",
    "        nn.init.normal_(m.weight.data, std=0.01)\n",
    "        # The bias parameter is initialized to zero by default.\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a1a71",
   "metadata": {},
   "source": [
    "## LOSS FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f87a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496c032",
   "metadata": {},
   "source": [
    "## Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6405dc1e",
   "metadata": {},
   "source": [
    "We use SGD with a learning rate of 0.1, just as in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933f056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4daadd",
   "metadata": {},
   "source": [
    "## Classication Accuracy\n",
    "Given a class of predicted probability distributions y_hat , we use the one with the highest predicted probability as the output category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
